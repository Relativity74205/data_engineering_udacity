{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "US immigration, demographics and temperature data is extracted, transformed and loaded into a Postgres DB to allow analysis of the us immigrations e.g. for touristic analysis.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import immigration\n",
    "import demographics\n",
    "import temperatures\n",
    "import database\n",
    "from queries import insert_into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: General Info\n",
    "\n",
    "The configuration for the project can be found in the `config.toml` file. Following settings can be defined there:\n",
    "\n",
    "- postgres connection settings\n",
    "- path to data files\n",
    "\n",
    "The used PostgresDB runs locally inside a docker container, see the provided `docker-compose.yml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "Explain what you plan to do in the project in more detail.\n",
    "\n",
    "- What data do you use?\n",
    "    - Several datasets are used to provide a base for analysis of immigration into the US:\n",
    "        - us immigration data\n",
    "        - us demographics data (on city and state level)\n",
    "        - international temperature data\n",
    "- What is your end solution look like?       \n",
    "    - After the extraction and transformation of the data, the data is loaded into a PostgresDB in a star schema (see the model below). The fact table consists of each immigrant. The dimensions table provide additional information, which allows to query the data to answer different questions:\n",
    "        - How many immigrations were made per day/week/month per state depending on the visa type?\n",
    "        - How does the amount of tourist immigrations vary with calendar month or temperature of target city/state?\n",
    "        - How does the amount of immigrations from severel world regions depend on the demographics of the target area?\n",
    "        - etc...\n",
    "- What tools did you use?\n",
    "    - Python functions were used for etl, a local PostgresDB inside a docker container was used as database (see the provided `docker-compose.yml`).\n",
    "\n",
    "### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "- US immigration data (from US National Tourism and Trade Office)\n",
    "    - each row describes a person immigrating inside the US\n",
    "    - additional information about the person and the immigration are provided, e.g. the port of the immigration, the age of the person, etc.\n",
    "    - many information in the raw immigration data is abbreviated by codes. An accompanying text files provides a key-value mapping for the codoes, e.g. port_code -> name of immigration port\n",
    "- US demographics data by city (from OpenSoft https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "    - describes basic demographics for the bigger US cities\n",
    "- US demographics data by state (from KFF https://www.kff.org/other/state-indicator/distribution-by-raceethnicity/?dataView=1&currentTimeframe=3)\n",
    "    - describes basic demographics for all US states\n",
    "- Internation temperature data (from Kaggle https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "    - describes the historic mean temperature for big international cities by city, year, month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 2: Explore and Assess the Data\n",
    "## Explore the Data and Cleaning Steps\n",
    "\n",
    "The result of the data exploration and the performed cleaning steps are briefly described below. The used functions are documented in the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration data\n",
    "\n",
    "Based on part of the data (approx. 3 million records) the following observations were made:\n",
    "\n",
    "#### Dropped columns\n",
    "\n",
    "The following columns are not used:\n",
    "\n",
    "- biryear (redundant because of i94bir)\n",
    "- entdepu (high number of nulls, unknown meaning)\n",
    "- entdepa (unknown meaning)\n",
    "- entdepd (unknown meaning)\n",
    "- count (no information)\n",
    "- matflag (meaning unknown)\n",
    "- occup (high number of nulls)\n",
    "- visapost (high number of nulls, no relevant information for the case)\n",
    "- insnum (high number of nulls, no information)\n",
    "- dtadfile (no relevant information for the case)\n",
    "- draddto (no relevant information for the case)\n",
    "- visatype (no relevant information for the case)\n",
    "- fltno (no relevant information for the case)\n",
    "- admnum (no information)\n",
    "- i94cit (a lot of codes - approx. 15 % - are not found in the regions list of the i94 description; in addition the exact meaning of the column is not known)\n",
    "\n",
    "#### null values\n",
    "\n",
    "- i94bir: <0.1% of values are NaN. Nulls cause problems with aggregation, therefore the field is imputed with the median\n",
    "- i94mode: <0.01% of i94mode values are NaN, a value of 9 has the meaning \"not reported\", therefore the NaN values are filled with 9\n",
    "- i94addr: approx 5% of values are NaN. While a value of '99' has the meaning \"others\", it cannot be used here as the reason why the high number of NAs are present is not known\n",
    "- gender: approx 15% of values are NaN, leave as is\n",
    "- airline: < 1%, leave as is\n",
    "- depdate: approx 5%, leave as is, the probable meaning is, that the immigrant has not departed yet\n",
    "\n",
    "i94addr, gender, airline, depdate are allowed to be nullable\n",
    "\n",
    "#### Additional cleaning steps\n",
    "\n",
    "- arrdate and depdate were converted from SAS date format\n",
    "- depdate: pandas NaT values were converted to None as psycopg2 does not know how to handle pandas NaT format\n",
    "\n",
    "\n",
    "### Accompanying immigration data\n",
    "\n",
    "The accompanying text file (I94_SAS_Labels_Descriptions.sas) was parsed the extract the following mappings:\n",
    "- port_code -> port\n",
    "    - the city, country and the us state was extracted from the port name\n",
    "- region_code -> region name\n",
    "    - in most cases the region is a single country\n",
    "- state_code -> state name\n",
    "    - it was enriched by the us state demographics data (see below)\n",
    "- visa_code -> kind of visa (e.g. business or pleasure)\n",
    "- travel_mode_code -> travel mode (e.g. air or sed)\n",
    "\n",
    "### Demographics data (city level)\n",
    "\n",
    "- some columns were not relevant and were not used:\n",
    "    - State (redundant)\n",
    "    - Number of Veterans (not relevant for this use case)\n",
    "    - Median Age (not relevant for this use case)\n",
    "    - Average Household Size (not relevant for this use case)\n",
    "    - Male Population (not relevant for this use case)\n",
    "    - Female Population (not relevant for this use case)\n",
    "- The data is provided partly as wide and partly as a long table. E.g. the total population has its own columns, however the race columns has multiple entries per city and race.\n",
    "Therefore the race columns was pivoted. The long table is transformed into a wide table for the population numbers per race.\n",
    "\n",
    "### Demographics data (state level)\n",
    "\n",
    "The data was used as is.\n",
    "\n",
    "### Temperature data\n",
    "\n",
    "- For US states no state_code was provided. However, the gps coordinates were provided, they were used to distinguish between US cities with the same name.\n",
    "- For very old timeframes (e.g. year 1900) many values were missing. However, only the temperature in the near past is relevant here.\n",
    "- Therefore the only the last three values for each city and calendar month were used to calculate the mean temperature for each city and calendar month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The model schema of the immigration table looks like the following:\n",
    "\n",
    "![DB_Schema](schemaspy/results/diagrams/summary/relationships.real.large.png)\n",
    "\n",
    "This model was chosen as the basic fact in this context is the immigration of one person, thus the _immigration_ table is defined as the fact table of the star schema. The other tables are dimensional tables, which provide additional information for the immigration process.\n",
    "\n",
    "Two additional tables (demographics and temperatures) are not shown in the relationsship diagram as no foreign key relationsship can be formed between the immigration tables and this two tables. Of course, there are columns on which the tables can be joined (city in case of the temperatures table, and city+state_code in case of the demographics table), however, in both tables there are entries, which are not present in the other one.\n",
    "\n",
    "![DB_Schema](schemaspy/results/diagrams/orphans/demographics.1degree.png)\n",
    "\n",
    "![DB_Schema](schemaspy/results/diagrams/orphans/temperatures.1degree.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model:\n",
    "- load the file with the accompanying immigration data, process it and save to the dimension tables\n",
    "- extract immigration data, transform it and save to the _immigration_ fact table\n",
    "- extract demographics data, transform it and save to the _demographics_ table\n",
    "- extract temperatures data, transform it and save to the _temperatures_ table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "The data model is created by etl functions in seperate python files. Please see the docstrings and the comments inside the files for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration.immigration_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.demographics_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures.temperatures_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "To ensure data quality, the following constraints were created in the DDL queries:\n",
    "- (compound) primary keys\n",
    "- foreign key relationships\n",
    "- data type checks\n",
    "- not null checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "see external [data dictionary markdown file](data_dictionary.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * The data size is medium, no fast access is needed, therefore no big data technologies like AWS Redshift, Apache Cassandra or Appache Spark were needed. Therefore a relational database like postgres is best for this case as it allows to include data quality checks in the definition of the tables.\n",
    "\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "    * Immigration data:\n",
    "        * Monthly, as the data is only slowly changing. But this depends on the exact use case. In some cases a daily update would be benefical.\n",
    "    * Temperature data:\n",
    "        * Yearly, as the data is aggregated on a monthly level and is only slowly changing.\n",
    "    * Demographics data:\n",
    "        * Yearly, as the data is only slowly changing\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x.\n",
    "        * Then a distributed system like Apache Spark is needed for the ETL process.\n",
    "    * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        * An Apache Airflow dag in an AWS fully managed airflow instance would be setup which starts the etl job on a daily basis e.g. on midnight.\n",
    "    * The database needed to be accessed by 100+ people.\n",
    "        * A big data database like AWS Redshift or Snowflake would be needed to store the data. Depending on the exact use case also Apache Cassandra would have to be used for predifined queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
